{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civil Comments - Classifying Toxicity, Using NLTK Naive Bayes Classifier\n",
    "\n",
    "## By: ZXS\n",
    "\n",
    "### Introduction-\n",
    "\n",
    "The Civil Comments Platform, which has since shut down, made its content of roughly 2 million user-comments available in the form of a dataset in 2017.\n",
    "\n",
    "These comments have all been classified and labelled with an index of toxicity representing the level of negative sentiment believed to be present in each individual post. The toxicity index is a continuous variable between 0 and 1 and comments can be classified by a value => .5 as toxic. \n",
    "\n",
    "Our objective is to see if it is possible to build a machine learning model that is able to accurately identify this sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure-\n",
    "\n",
    "In order to conduct a sentiment analysis, we will first have to perform some \"natural language processing\" on our datta. There are many different ways that this can be done. For our purposes we will make the following modifications to our data:\n",
    "\n",
    "* Remove punctuation.\n",
    "* **Tokeninzing:** splitting all paragraphs/sentences into lists of the component words.\n",
    "* Removing stopwords.\n",
    "* **Stemming:** the removal of all endings from words leaving only the root. For example, \"running\" would become \"run\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "wd = '/Users/zxs/Documents/code/kaggle/sentiment/'\n",
    "os.chdir(wd)\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('train.csv.zip', compression = 'zip')\n",
    "test_df = pd.read_csv('test.csv.zip', compression = 'zip')\n",
    "\n",
    "# Load the pickled data\n",
    "with open('sentiment.pickle', mode = 'rb') as f:\n",
    "\n",
    "    train_text = pickle.load(f)\n",
    "\n",
    "f.close() \n",
    "\n",
    "with open('train_words.pickle', mode = 'rb') as f:\n",
    "\n",
    "    train_words = pickle.load(f)\n",
    "\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804874/1804874 [00:01<00:00, 1064537.36it/s]\n",
      "100%|██████████| 1804874/1804874 [00:03<00:00, 548007.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Rejoin the data\n",
    "train_df['processed_text'] = train_words\n",
    "\n",
    "# Split the data based on toxicity\n",
    "train_df['toxicity'] = train_df['target'].progress_apply(lambda x: 'positive' if x >= .5 else 'negative')\n",
    "\n",
    "# Arrange list of words and labels\n",
    "train_df['processed_text'] = train_df['processed_text'].progress_apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Separate\n",
    "text = train_df[['processed_text', 'toxicity']].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction-\n",
    "\n",
    "Now that the comments have been separated into components and processed, we must extract features that belong to each different class of toxicity- \"positive\" and \"negative\" based on having a value for \"target\" (representing the continuous value between 0 and 1 assigned by the human scoring).\n",
    "\n",
    "To do this, we will create simple dictionaries for each class of comments containing boolean values for whether or not the word is present in the texts belonging to each respective class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-774cfcc1c934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mnbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/classify/util.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(classifier, gold)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/classify/api.py\u001b[0m in \u001b[0;36mclassify_many\u001b[0;34m(self, featuresets)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/classify/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Otherwise, we'll just assign a probability of 0 to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# everything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Function to process features\n",
    "def word_fts(words):\n",
    "    \n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Split the data for training and testing\n",
    "train, test = train_test_split(text, test_size = .2, random_state = 100)    \n",
    "\n",
    "# Iterate the tokenized data to extract features\n",
    "toxic_train = []\n",
    "nontoxic_train = []\n",
    "\n",
    "for i in train:\n",
    "    \n",
    "    if i[1] == 'positive':\n",
    "        \n",
    "        toxic_train.append(word for word in word_tokenize(i[0]))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        nontoxic_train.append(word for word in word_tokenize(i[0]))\n",
    "\n",
    "toxic_test = []\n",
    "nontoxic_test = []\n",
    "        \n",
    "# Remove duplicates        \n",
    "toxic_train = set(toxic_train)\n",
    "nontoxic_train = set(nontoxic_train)\n",
    "\n",
    "# ID features\n",
    "toxicft = [(word_fts(tox), 'positive') for tox in toxic_train]\n",
    "nontoxicft = [(word_fts(nontox), 'negative') for nontox in nontoxic_train]\n",
    "\n",
    "# Recombine features\n",
    "train = toxicft + nontoxicft\n",
    "\n",
    "# Model\n",
    "nbc = NaiveBayesClassifier.train(train)\n",
    "\n",
    "# Evaluate\n",
    "print ('Accuracy:', nltk.classify.util.accuracy(nbc, test))\n",
    "nbc.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
