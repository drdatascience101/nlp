{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Data Science 101: NLP\n",
    "\n",
    "## Latent Semantic Analysis\n",
    "\n",
    "### Introduction-\n",
    "\n",
    "In the real world, there are a variety of different ways that we as humans choose to organize our things. For example, if youre a fan of books then you might choose to file yours away alphabetically on your bookshelf. Or else, you might choose instead to leave them strewn about in piles according to subject. \n",
    "\n",
    "Similarly, computers must also find ways to store and retrieve unstructured data while retaining the ability to quickly access information when needed. \n",
    "\n",
    "**Latent semantic analysis (LSA)** is one such method of indexing unstructured electronic text data.\n",
    "\n",
    "This is a method of unsupervised learning. There is no target variable. Instead, we are trying to mine documents for similarities. \n",
    "\n",
    "This can be done through a combination of three different pre-processing steps:\n",
    "\n",
    "    1) Determine frequency of terms in document\n",
    "    \n",
    "        2) Determine inverse frequency of terms in document across all documents\n",
    "        \n",
    "            3) **Singular Value Decomposition**\n",
    "            \n",
    "We will not get into the technical details of this procedure but instead focus on its implementation in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing-\n",
    "\n",
    "Extracting features and attributes from unstructured data is no easy task. Before we can make sense of the information at hand, we must first process it so that the data can be more easily ingested by the computer. \n",
    "\n",
    "This entails several steps. These steps do not necessarily need to be performed in any specific order, which is actually dictated by the constraints of the programming language and style of the coder. Regardless, some important components are: **removing punctuation / special characters**,\n",
    "                **removing \"stop\" words**,\n",
    "                **\"stemming\"**.\n",
    "\n",
    "In addition to these steps, there are other optional pre-processing methods that can additionally impact accuracy and results of any model, these include: *n-gram counting, lemmatizing, etc.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "wd = '/Users/zxs/Documents/code/kaggle/sentiment/'\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a small subset of data for demonstration\n",
    "df = pd.read_csv('train.csv.zip', compression = 'zip')\n",
    "df = df.sample(frac = .1, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stem the words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Fix the casing\n",
    "text = [i.lower() for i in df['comment_text']]\n",
    "\n",
    "# Remove punctuation\n",
    "no_punct = [i.translate(str.maketrans('', '', string.punctuation)) for i in text]\n",
    "\n",
    "# Tokenize the words\n",
    "tokens = [word_tokenize(x) for x in no_punct]\n",
    "\n",
    "# Remove stop words\n",
    "no_stops = []\n",
    "\n",
    "for i in tokens:\n",
    "    \n",
    "    no_stops.append([x for x in i if x not in stop_words])\n",
    "    \n",
    "# Stemming\n",
    "stems = []\n",
    "\n",
    "for i in no_stops:\n",
    "    \n",
    "    stemmed = [ps.stem(x) for x in i]\n",
    "\n",
    "    stems.append(stemmed)\n",
    "\n",
    "# Rejoin content\n",
    "joined = []\n",
    "    \n",
    "for i in range(len(stems)):\n",
    "    \n",
    "    t = ' '.join(stems[i])\n",
    "    joined.append(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
